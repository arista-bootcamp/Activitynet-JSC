{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import video_input as vi\n",
    "import model as model\n",
    "import utils as utils\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = utils.yaml_to_dict('../config.yml')\n",
    "params['data_dir'] = os.path.join('..', params['data_dir'])\n",
    "params['videos_folder'] = os.path.join('..', params['videos_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 3,\n",
       " 'classes_amount': 101,\n",
       " 'data_dir': '../data',\n",
       " 'eval_steps': 10,\n",
       " 'json_data_path': 'data/activity_net.v1-2.min.json',\n",
       " 'json_metadata_path': 'data/training_meta_data.json',\n",
       " 'keep_checkpoint_max': 3,\n",
       " 'learning_rate': 0.0001,\n",
       " 'log_step_count_steps': 20,\n",
       " 'max_steps': 21000,\n",
       " 'model': 'gap',\n",
       " 'model_dir': '.temp/checkpoints',\n",
       " 'num_epochs': 1000,\n",
       " 'resize': [224, 224],\n",
       " 'save_checkpoints_steps': 20,\n",
       " 'save_summary_steps': 20,\n",
       " 'shuffle': True,\n",
       " 'skip_frames': 10,\n",
       " 'start_delay_secs': 10,\n",
       " 'temp_dir': '.temp',\n",
       " 'throttle_secs': 10,\n",
       " 'videos_folder': '../data/videos',\n",
       " 'weight_factor': 8}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Size (2448, 3264, 3)\n",
      "Resized and normalized image (1, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "img_path = os.path.join('..',params['temp_dir'], 'test_img.jpg')\n",
    "img_width = 224\n",
    "img_height = 224\n",
    "\n",
    "test_img = cv2.imread(img_path)\n",
    "print('Original Size',test_img.shape)\n",
    "\n",
    "img_resize = cv2.resize(test_img,(img_width,img_height))\n",
    "img_resize = img_resize[None, ...]\n",
    "\n",
    "img_normalized = (img_resize / 255.0)\n",
    "print('Resized and normalized image', img_normalized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pretrain_model = model._initialize_pretrained_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions with different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params['json_data_path'] = os.path.join('..',params['json_data_path'])\n",
    "params['json_metadata_path'] = os.path.join('..',params['json_metadata_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_gen = vi.all_data_videos(params)\n",
    "particular_frames = next(video_gen)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "particular_frames = particular_frames[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "block4_pool_features = pretrain_model.predict(particular_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 5, 1536)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block4_pool_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = model.gap_module(block4_pool_features)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    elements = sess.run(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5006858 , 0.49980167, 0.49955243, 0.5017479 , 0.5001104 ,\n",
       "       0.5020521 , 0.5002102 , 0.5001207 , 0.50145364, 0.49953502,\n",
       "       0.50092816, 0.502103  , 0.4986282 , 0.49977246, 0.49872294,\n",
       "       0.5001548 , 0.4990335 , 0.5000021 , 0.50046325, 0.49861667,\n",
       "       0.50012636, 0.4997803 , 0.50031716, 0.50005025, 0.4995632 ,\n",
       "       0.49796578, 0.50134337, 0.49940774, 0.49767587, 0.5003512 ,\n",
       "       0.4996356 , 0.49937448, 0.49948993, 0.5000551 , 0.5019626 ,\n",
       "       0.5012438 , 0.49845654, 0.4976322 , 0.50193685, 0.49563825,\n",
       "       0.4990612 , 0.49995482, 0.50074804, 0.50087565, 0.4993557 ,\n",
       "       0.498907  , 0.5002777 , 0.50050974, 0.50073975, 0.5006029 ,\n",
       "       0.49967209, 0.50180435, 0.5005373 , 0.49922538, 0.5002415 ,\n",
       "       0.50008035, 0.50154585, 0.4981932 , 0.4986977 , 0.49672168,\n",
       "       0.5001936 , 0.5020308 , 0.49882108, 0.5001076 , 0.5005728 ,\n",
       "       0.49986482, 0.50095636, 0.5002811 , 0.49961215, 0.49920332,\n",
       "       0.5035722 , 0.50091654, 0.49852088, 0.49961802, 0.5006299 ,\n",
       "       0.50041884, 0.49895918, 0.49922448, 0.50035095, 0.499241  ,\n",
       "       0.5001596 , 0.49977863, 0.5026409 , 0.49888253, 0.5003424 ,\n",
       "       0.5007841 , 0.5007435 , 0.50122505, 0.5002659 , 0.49942347,\n",
       "       0.50070727, 0.5011089 , 0.50017434, 0.49931148, 0.5006302 ,\n",
       "       0.5005641 , 0.4994177 , 0.50166327, 0.5008479 , 0.50125515,\n",
       "       0.5021759 ], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements[0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = model.dense_module(block4_pool_features)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    elements = sess.run(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6181099 ,  0.774728  , -0.62187123, -0.9923966 , -0.02499747,\n",
       "        0.30077046, -0.32329673, -0.21192867,  0.76027584,  0.11780857,\n",
       "       -1.2356801 , -0.26320225,  0.36232397,  0.28695464, -0.22462136,\n",
       "        0.64828837, -0.6722048 ,  0.92188233,  0.47824645,  0.14497358,\n",
       "       -1.1362485 , -0.42516565, -0.825943  , -0.7148176 , -0.68884337,\n",
       "       -0.6185166 , -0.01969439,  0.31619963, -0.54573697,  0.12805745,\n",
       "        1.3144857 , -0.5395431 ,  0.43202168, -0.7889519 ,  0.79659104,\n",
       "       -0.2636819 , -0.5107892 , -0.09784085,  0.14369413,  0.16350421,\n",
       "       -0.54978734, -0.46457025, -0.34689838, -0.61426395, -0.05663039,\n",
       "       -0.58806545, -0.04766598, -0.00789861,  0.84846556, -0.16402294,\n",
       "       -0.95529836, -0.37164813, -0.5013966 , -0.04599807, -0.15904763,\n",
       "       -0.3082617 , -0.38378876, -0.9938363 ,  0.06937587,  0.48646542,\n",
       "       -0.516217  ,  0.28221872, -0.82786745, -0.26739353,  0.70640063,\n",
       "        0.31455636,  0.45440435, -0.33601806, -0.84215873,  0.60594815,\n",
       "       -0.47248822, -0.47594982,  0.8859411 ,  0.08715497,  0.9022442 ,\n",
       "       -0.25893536, -0.27920824, -0.33156878,  1.2713615 ,  0.657383  ,\n",
       "       -0.09603018, -0.07630864, -0.02294616, -0.59491694, -0.24523306,\n",
       "       -0.19925882, -0.49916676, -0.21020052, -0.29258227, -1.0443003 ,\n",
       "        0.76246125, -0.9031601 , -1.0131769 ,  0.0806853 ,  0.19527943,\n",
       "        1.4447659 , -0.22468904,  0.03432371, -0.38904443,  0.3626188 ,\n",
       "       -0.6836635 ], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements[0,...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data and make logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_videos = vi.all_data_videos(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "block4_pool_features = pretrain_model.predict(particular_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5, 5, 1536) (3, 101)\n",
      "(3, 5, 5, 1536) (3, 101)\n"
     ]
    }
   ],
   "source": [
    "videos_dict = dict()\n",
    "\n",
    "for data in all_data_videos:\n",
    "    \n",
    "    image_frames = data[0]\n",
    "    image_label = data[1]\n",
    "    \n",
    "    image_feature_map = pretrain_model.predict(image_frames)\n",
    "    \n",
    "    videos_dict['feature_map'] = pretrain_model.predict(image_frames)\n",
    "    videos_dict['label'] = image_label\n",
    "    \n",
    "    print(videos_dict['feature_map'].shape,videos_dict['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
