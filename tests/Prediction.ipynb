{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append('..')\n",
    "import data\n",
    "import model\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 15,\n",
       " 'buffer_size': 50,\n",
       " 'cell_type': 'lstm',\n",
       " 'classes_amount': 34,\n",
       " 'data_dir': '../data',\n",
       " 'data_generator': 'windows',\n",
       " 'dropout': 0.3,\n",
       " 'eval_steps': 10,\n",
       " 'feature_map_folder': '../data/feature_maps',\n",
       " 'feature_maps_size': [5, 5, 1536],\n",
       " 'json_data_path': '../data/activity_net.v1-2.min.json',\n",
       " 'json_metadata_path': '../data/training_meta_data_reduced.json',\n",
       " 'keep_checkpoint_max': 3,\n",
       " 'label_feature_maps_size': [101],\n",
       " 'learning_rate': 1e-05,\n",
       " 'log_step_count_steps': 20,\n",
       " 'max_frames': [15],\n",
       " 'max_steps': 21000,\n",
       " 'model': 'gap',\n",
       " 'model_dir': '../.temp/checkpoints',\n",
       " 'num_epochs': 500,\n",
       " 'num_layers': 1,\n",
       " 'num_nodes': 2,\n",
       " 'predict_mode': 'mode',\n",
       " 'resize': [224, 224],\n",
       " 'save_checkpoints_steps': 20,\n",
       " 'save_summary_steps': 20,\n",
       " 'shuffle': True,\n",
       " 'skip_frames': 6,\n",
       " 'start_delay_secs': 10,\n",
       " 'taxonomy_level': 2,\n",
       " 'temp_dir': '.temp',\n",
       " 'throttle_secs': 10,\n",
       " 'videos_folder': '../data/videos',\n",
       " 'weight_factor': 8,\n",
       " 'window_size': 15}"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = utils.yaml_to_dict('../config.yml')\n",
    "params['data_dir'] = os.path.join('..', params['data_dir'])\n",
    "params['model_dir'] = os.path.join('..', params['model_dir'])\n",
    "params['videos_folder'] = os.path.join('..', params['videos_folder'])\n",
    "params['feature_map_folder'] = os.path.join('..', params['feature_map_folder'])\n",
    "params['json_data_path'] = os.path.join('..', params['json_data_path'])\n",
    "params['json_metadata_path'] = os.path.join('..', params['json_metadata_path'])\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(params['json_metadata_path']) as data_file:\n",
    "    metadata_json = json.load(data_file)\n",
    "\n",
    "label_by_idx = {\n",
    "    'level_3': {},\n",
    "    'level_2': {},\n",
    "    'level_1': {},\n",
    "    'level_0': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, item in metadata_json.items():\n",
    "    if key != 'classes_amount':\n",
    "        label_by_idx['level_3'][item['idx']] = key\n",
    "        for level in ['level_2', 'level_1', 'level_0']:\n",
    "            label_by_idx[level][item[level]['idx']] = item[level]['name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_gen_test = data.DataGenerator(params, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = tf.estimator.Estimator(\n",
    "    # Custom model function\n",
    "    model_fn=model.model_fn,\n",
    "    params=params,\n",
    "    # Model directory\n",
    "    model_dir=params['model_dir'],\n",
    "    # warm_start_from=cfg.PRE_TRAIN,\n",
    "    config=tf.estimator.RunConfig(\n",
    "        keep_checkpoint_max=params['keep_checkpoint_max'],\n",
    "        save_checkpoints_steps=params['save_checkpoints_steps'],\n",
    "        save_summary_steps=params['save_summary_steps'],\n",
    "        log_step_count_steps=params['log_step_count_steps']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = estimator.predict(\n",
    "    input_fn = lambda: data.input_fn(data_gen_test, False, params)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_results = {\n",
    "    \"results\": {}\n",
    "}\n",
    "available_formats = ['.mkv', '.webm', '.mp4'] \n",
    "predictions_by_video = {}\n",
    "\n",
    "for item in predictions:\n",
    "    video_id = item['metadata'].decode('utf-8')\n",
    "    batch_num = int(video_id.split('batch')[-1].replace('_', ''))\n",
    "    video_id = video_id.split('batch')[0].replace('_', '')\n",
    "    \n",
    "    for vformat in available_formats:\n",
    "        video_path = os.path.join(params['videos_folder'] + '/validation', video_id + vformat)\n",
    "        if os.path.isfile(video_path):\n",
    "            break\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    if fps == 0.0:\n",
    "        continue\n",
    "    \n",
    "    if video_id not in prediction_results['results']:\n",
    "        prediction_results['results'][video_id] = {}\n",
    "\n",
    "    frame_difference = (params['skip_frames'] / fps)\n",
    "    frame_id = 0\n",
    "\n",
    "    classes_pred = np.ones(item['probabilities'].shape[0]) * item['classes']\n",
    "    classes_score = np.ones(item['probabilities'].shape[0]) * item['score']\n",
    "\n",
    "    for frame_pred in classes_pred:\n",
    "        frame_number = (params['batch_size'] * (batch_num - 1) + frame_id) * params['skip_frames']\n",
    "        frame_seconds = frame_number / fps\n",
    "        if frame_pred != 0:\n",
    "            if frame_pred not in prediction_results['results'][video_id]:\n",
    "                prediction_results['results'][video_id][frame_pred] = {\n",
    "                    \"seconds_array\": [frame_seconds],\n",
    "                    \"score\": item['score']\n",
    "                }\n",
    "            else:\n",
    "                prediction_results['results'][video_id][frame_pred][\"seconds_array\"].append(frame_seconds)\n",
    "        frame_id += 1\n",
    "\n",
    "prediction_format = {\n",
    "    \"results\": {}\n",
    "}\n",
    "\n",
    "for key, item in prediction_results['results'].items():\n",
    "    prediction_format['results'][key] = []\n",
    "    for classes_pred, classes_pred_info in item.items():\n",
    "        sec_arr = np.array(classes_pred_info['seconds_array'])\n",
    "        sec_arr = np.sort(sec_arr)\n",
    "        ini_sec = sec_arr[0]\n",
    "        sequences = [[ini_sec, ini_sec]]\n",
    "        for idx in range(1, sec_arr.shape[0]):\n",
    "            if sec_arr[idx] - ini_sec > 1.0:\n",
    "                sequences[-1][1] = ini_sec\n",
    "                sequences.append([sec_arr[idx], sec_arr[idx]])\n",
    "            else:\n",
    "                sequences[-1][1] = sec_arr[idx]\n",
    "            ini_sec = sec_arr[idx]\n",
    "\n",
    "        for seq_item in sequences:\n",
    "            to_insert = {\n",
    "                    'score': classes_pred_info['score'].item(),\n",
    "                    'segment': [seq_item],\n",
    "                    'label': label_by_idx['level_' + str(params['taxonomy_level'])][int(classes_pred)]\n",
    "            }\n",
    "            if key not in prediction_format['results']:\n",
    "                    prediction_format['results'][key] = [to_insert]\n",
    "            else:\n",
    "                prediction_format['results'][key].append(to_insert)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': {'-8awLlFLcQc': []}}"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/predicted_output.json', 'w') as outfile:\n",
    "    json.dump(prediction_format, outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction_format['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
